





You said:
import pandas as pd

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/churn_prediction.csv")

# Display the first few rows
df.head()
import matplotlib.pyplot as plt
import seaborn as sns

# Check dataset structure
print(df.info())
print(df.describe())

print(df.shape)
print(df.dtypes)

# Check missing values
print(df.isnull().sum())
# churn_counts = df['churn'].value_counts()
# total_samples = len(df['churn'])

# sns.barplot(x=churn_counts.index, y=churn_counts.values)
# plt.title('Churn Statistic')
# plt.xlabel('Churn')
# plt.ylabel('Count')

# for index, value in enumerate(churn_counts):
#     plt.text(index, value + 100, f'{value} ({value / total_samples * 100:.2f}%)', ha='center', va='bottom')

# plt.xticks(ticks=[0, 1], labels=['Not Churn', 'Churn'])
# plt.show()

# Visualize churn distribution
sns.countplot(x='churn', data=df)
plt.title("Churn Distribution")
plt.xticks(ticks=[0, 1], labels=['Not Churn', 'Churn'])
plt.show()
# Age distribution visualization

age_counts = df['age']
plt.figure(figsize=(10, 6))
sns.histplot(age_counts, bins=20, kde=True, color='skyblue')
plt.title("Age Distribution")
plt.xlabel('Age')
plt.ylabel('Count')
total_samples = len(age_counts)
plt.show()
# Gender statistic

print(df['gender'].value_counts(dropna=False))


missing_gender_percentage = df['gender'].isnull().sum() / len(df) * 100
print(f"Percentage of missing values in 'gender': {missing_gender_percentage:.2f}%")
# Replace missing values with 'Unknown'

df['gender'].fillna('Unknown', inplace=True)
print(df['gender'].value_counts(dropna=False))
# Gender visualization

gender_counts = df['gender'].value_counts(dropna=False)
total_samples = len(df['gender'])

sns.barplot(x=gender_counts.index, y=gender_counts.values)
plt.title('Gender Statistic')
plt.xlabel('Gender')
plt.ylabel('Count')

for index, value in enumerate(gender_counts):
    if index == 2:  # To avoid placing label for NaN values
        continue
    plt.text(index, value + 100, f'{value} ({value / total_samples * 100:.2f}%)', ha='center', va='bottom')

plt.xticks(ticks=[0, 1, 2], labels=['Male', 'Female', 'NaN'])
plt.show()
print(df['dependents'].unique())
# Dependents – find outliers

high_dependents = df[df['dependents'] > 10]
count_high_dependents = len(high_dependents)
print(f"The number of cases with more than 10 dependents: {count_high_dependents}")
# imputation with median

median_dependents = df[df['dependents'] <= 10]['dependents'].median()

df.loc[df['dependents'] > 10, 'dependents'] = median_dependents

df['dependents'].fillna(median_dependents, inplace=True)


print(df['dependents'].isnull().sum())
print(len(df[df['dependents'] > 10]))
# Occupation – statistic

print(df['occupation'].value_counts(dropna=False))
# Occupation – handle with missing values - replace missing values with 'Unknown'

df['occupation'].fillna('Unknown', inplace=True)

print(df['occupation'].value_counts(dropna=False))
# Occupation – visualization

occupation_counts = df['occupation'].value_counts(dropna=False)
plt.figure(figsize=(8, 6))
plt.pie(occupation_counts, labels=occupation_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Occupation Distribution')
plt.axis('equal')
plt.show()
# City – statistic

print(df['city'].value_counts(dropna=False))
# City – handle with missing values – replace with ’Unknown’

df['city'].fillna('Unknown', inplace=True)

print(df['city'].value_counts(dropna=False))
# City -  visualization customer count

top_ten_cities = df['city'].value_counts().nlargest(10)
plt.figure(figsize=(10, 6))
top_ten_cities.plot(kind='bar', color='skyblue')
plt.title('Top 10 Cities by Customer Count')
plt.xlabel('City')
plt.ylabel('Number of Customers')
plt.xticks(rotation=45)
plt.show()
# City -  visualization churn

top_ten_cities_churn = df[df['churn'] == 1]['city'].value_counts().nlargest(10)
plt.figure(figsize=(10, 6))
top_ten_cities_churn.plot(kind='bar', color='salmon')
plt.title('Top 10 Cities by Churn Count')
plt.xlabel('City')
plt.ylabel('Churn Count')
plt.xticks(rotation=45)
plt.show()
# City -  visualization churn percentage

total_churned = df['churn'].sum()
top_ten_cities_churn = df[df['churn'] == 1]['city'].value_counts(normalize=True).nlargest(10) * 100

plt.figure(figsize=(10, 6))
top_ten_cities_churn.plot(kind='bar', color='red')
plt.title('Top 10 Cities by Churn Percentage')
plt.xlabel('City')
plt.ylabel('Churn Percentage')
plt.xticks(rotation=45)
plt.yticks(range(0, 101, 10))
plt.show()
print(df['customer_nw_category'].value_counts())
# correlation between 'customer_nw_category' and 'current_balance'

correlation = df['customer_nw_category'].corr(df['current_balance'])
print(f"Correlation between customer_nw_category and current_balance: {correlation}")
# correlation between 'customer_nw_category' and 'churn'

correlation_churn_nw = df['customer_nw_category'].corr(df['churn'])
print(f"Correlation between customer_nw_category and churn: {correlation_churn_nw}")
# Visualization

category_counts = df['customer_nw_category'].value_counts()

category_percentages = (category_counts / category_counts.sum()) * 100

colors = ['lightblue', 'lightgreen', 'lightcoral']

plt.figure(figsize=(8, 6))
category_percentages.plot(kind='bar', color=colors)
plt.title('Customer NW Category Distribution')
plt.xlabel('Customer NW Category')
plt.ylabel('Percentage')
plt.xticks(rotation=0)
plt.ylim(0, 100)
for i, value in enumerate(category_percentages):
    plt.text(i, value + 1, f"{value:.2f}%", ha='center', va='bottom', fontsize=9)
plt.show()

# relation with churn

churned_data = df[df['churn'] == 1]

churn_by_category = churned_data['customer_nw_category'].value_counts().sort_index()

plt.figure(figsize=(10, 6))
sns.barplot(x=churn_by_category.index, y=churn_by_category.values, palette='viridis')

plt.title('Churn Counts by Customer Network Category')
plt.xlabel('Customer Network Category')
plt.ylabel('Churn Count')
plt.xticks(rotation=0)

plt.show()

# Converting ‘last transaction’ into days since last transaction

df['last_transaction'] = pd.to_datetime(df['last_transaction'])

current_date = pd.to_datetime('today')
df['days_since_last_transaction'] = (current_date - df['last_transaction']).dt.days

print(df[['customer_id', 'last_transaction', 'days_since_last_transaction']].head())
# check

df.head()
#  missing values in 'days_since_last_transaction'

missing_values_count = df['days_since_last_transaction'].isnull().sum()
print(f"Number of missing values in 'days_since_last_transaction': {missing_values_count}")

# Impute missing values with the mean

mean_days = df['days_since_last_transaction'].mean()
df['days_since_last_transaction'].fillna(mean_days, inplace=True)
#  percentage change in balances

df['percentage_change_balance'] = ((df['current_month_balance'] -
                                      df['previous_month_balance']) /
                                      df['previous_month_balance']) * 100
# total credit and debit amounts for each customer

df['total_credit'] = df['current_month_credit'] + df['previous_month_credit']
df['total_debit'] = df['current_month_debit'] + df['previous_month_debit']

# credit-to-debit ratio

df['credit_to_debit_ratio'] = df['total_credit'] / df['total_debit']
df['savings_ratio'] = df['current_month_balance'] / df['previous_month_balance']
# Exploring customer id to understand if there is more transaction
# of one customer but the category contains only unique values.

customer_id_counts = df['customer_id'].value_counts()
print(customer_id_counts)
df['credit_to_balance_ratio'] = df['current_month_credit'] / df['current_month_balance']
# Percentage Balance Change Q1 to Q2
df['balance_change_Q1_to_Q2'] = df['average_monthly_balance_prevQ'] - df['average_monthly_balance_prevQ2']
df['percentage_balance_change_Q1_to_Q2'] = ((df['average_monthly_balance_prevQ'] - df['average_monthly_balance_prevQ2']) / df['average_monthly_balance_prevQ2']) * 100
# Balance Change Q1 to Q2

df['balance_comparison_Q1_to_current'] = df['current_month_balance'] - df['average_monthly_balance_prevQ']

df['balance_comparison_Q2_to_current'] = df['current_month_balance'] - df['average_monthly_balance_prevQ2']
# data with newly engineered features

print(df[['balance_change_Q1_to_Q2', 'percentage_balance_change_Q1_to_Q2',
            'balance_comparison_Q1_to_current', 'balance_comparison_Q2_to_current']].head())
#  all  features
print(df.head())
# check
print(df.dtypes)
# Encoding categories ‘gender’ and ‘occupation’ with one hot encoding due to low cardinality.

data_encoded = pd.get_dummies(df, columns=['gender', 'occupation'], drop_first=True)
# Frequency encoding to ‘city’ category due to high cardinality

city_freq = df['city'].value_counts(normalize=True)
data_encoded['city_freq_encoded'] = df['city'].map(city_freq)
columns_to_drop = [ 'city']
data_encoded.drop(columns_to_drop, axis=1, inplace=True)
# Impute missing values with the mean or median
mean_days = df['days_since_last_transaction'].mean()
median_days = df['days_since_last_transaction'].median()

# Replace missing values with the mean
df['days_since_last_transaction'].fillna(mean_days, inplace=True)
# Or replace with the median
# df['days_since_last_transaction'].fillna(median_days, inplace=True)
# Drop the original 'last_transaction' column

columns_to_drop = [ 'last_transaction']
data_encoded.drop(columns_to_drop, axis=1, inplace=True)
# Train test split
from sklearn.model_selection import train_test_split


X = data_encoded.drop('churn', axis=1)  # Features
y = data_encoded['churn']  # Target variable


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Check
print(X_train.dtypes)
print(X_test.dtypes)
# Models
# Logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

logistic_model = LogisticRegression(max_iter=1000)

logistic_model.fit(X_train, y_train)

y_pred = logistic_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))
accuracy_scores = {}
precision_churn = {}
recall_churn={}
f1_churn={}
# Add results to dictionary in order to visualize

accuracy_scores['Logistic Regression']= 0.83
precision_churn['Logistic Regression'] = 0.76
recall_churn['Logistic Regression']= 0.10
f1_churn['Logistic Regression']= 0.17
# Visualisation


from sklearn.metrics import confusion_matrix

# Your confusion matrix values
tn, fp, fn, tp = 4607, 32, 939, 99
cm = [[tn, fp], [fn, tp]]

# Plot confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Logistic Regression')
plt.show()
# Feature importances

coefficients = abs(logistic_model.coef_[0])
feature_importance_logistic = pd.DataFrame({
    'Feature': X.columns,
    'Importance': coefficients
})
feature_importance_logistic = feature_importance_logistic.sort_values(by='Importance', ascending=False)
print(feature_importance_logistic)
# Random forest

from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest.fit(X_train, y_train)
y_pred = random_forest.predict(X_test)
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
# Add results to dictionary in order to visualize

accuracy_scores['Random forest']= 0.869
precision_churn['Random forest'] = 0.73
recall_churn['Random forest']= 0.45
f1_churn['Random forest']= 0.56
# Visualisation

tn_rf, fp_rf, fn_rf, tp_rf = 4462, 177, 564, 474
cm_rf = [[tn_rf, fp_rf], [fn_rf, tp_rf]]

plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Reds')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()
# Feature importances

feature_importance_rf = pd.DataFrame({
    'Feature': X.columns,
    'Importance': random_forest.feature_importances_
})
feature_importance_rf = feature_importance_rf.sort_values(by='Importance', ascending=False)
print(feature_importance_rf)
# GradientBoost

from sklearn.ensemble import GradientBoostingClassifier

gradient_boosting = GradientBoostingClassifier()

gradient_boosting.fit(X_train, y_train)

y_pred_gb = gradient_boosting.predict(X_test)

accuracy_gb = accuracy_score(y_test, y_pred_gb)
print(f"Accuracy: {accuracy_gb:.5f}")

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
# Add results to dictionary in order to visualize

accuracy_scores['Gradient boosting']= 0.867
precision_churn['Gradient boosting'] = 0.73
recall_churn['Gradient boosting']= 0.45
f1_churn['Gradient boosting']= 0.56
# XGBoost

from xgboost import XGBClassifier

xgb_classifier = XGBClassifier()

xgb_classifier.fit(X_train, y_train)

y_pred_xgb = xgb_classifier.predict(X_test)

accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy: {accuracy_xgb:.5f}")

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
# Add results to dictionary in order to visualize

accuracy_scores['XGBoost']= 0.8607
precision_churn['XGBoost'] = 0.73
recall_churn['XGBoost']= 0.45
f1_churn['XGBoost']= 0.56
# Visualisation

tn_xgb, fp_xgb, fn_xgb, tp_xgb = 4462, 177, 564, 474
cm_xgb = [[tn_xgb, fp_xgb], [fn_xgb, tp_xgb]]


plt.figure(figsize=(6, 4))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Reds')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - XGBClassifier')
plt.show()
# Feature importances

xgb_classifier.fit(X_train, y_train)
importance = xgb_classifier.get_booster().get_score(importance_type='weight')
feature_importance_xgb = pd.DataFrame({
    'Feature': list(importance.keys()),
    'Importance': list(importance.values())
})
feature_importance_xgb = feature_importance_xgb.sort_values(by='Importance', ascending=False)
print(feature_importance_xgb)
# XGBoost with GridSearchCV

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

xgb_model = XGBClassifier()

param_grid = {
    'learning_rate': [0.1, 0.01, 0.001],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300],
}

grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)
class_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(class_report)
# Add results to dictionary in order to visualize

accuracy_scores['XGBoost grid']= 0.8669
precision_churn['XGBoost grid'] = 0.44
recall_churn['XGBoost grid']= 0.47
f1_churn['XGBoost grid']= 0.45
# LightGBM classifier

from lightgbm import LGBMClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

lgbm = LGBMClassifier()

lgbm.fit(X_train, y_train)

y_pred = lgbm.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

class_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(class_report)
accuracy_scores['LightGBM']= 0.865
precision_churn['LightGBM'] = 0.70
recall_churn['LightGBM']= 0.46
f1_churn['LightGBM']= 0.55
# Feature importances

importance = lgbm.feature_importances_
feature_importance_lgbm = pd.DataFrame({
    'Feature': lgbm.feature_name_,
    'Importance': importance
})
feature_importance_lgbm = feature_importance_lgbm.sort_values(by='Importance', ascending=False)
print(feature_importance_lgbm)
# Adaptive boosting Classifier

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

adaboost_classifier = AdaBoostClassifier()

adaboost_classifier.fit(X_train, y_train)

y_pred_adaboost = adaboost_classifier.predict(X_test)

accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
print(f"Accuracy: {accuracy_adaboost}")

conf_matrix_adaboost = confusion_matrix(y_test, y_pred_adaboost)
print("Confusion Matrix:")
print(conf_matrix_adaboost)

classification_rep_adaboost = classification_report(y_test, y_pred_adaboost)
print("Classification Report:")
print(classification_rep_adaboost)
accuracy_scores['AdaBoost']= 0.857
precision_churn['AdaBoost'] = 0.69
recall_churn['AdaBoost']= 0.40
f1_churn['AdaBoost']= 0.51
# DecisionTree
from sklearn.tree import DecisionTreeClassifier

dt_classifier = DecisionTreeClassifier()

dt_classifier.fit(X_train, y_train)

y_pred = dt_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))
# Add results to dictionary in order to visualize

accuracy_scores['DecisionTree']= 0.80
precision_churn['DecisionTree'] = 0.45
recall_churn['DecisionTree']= 0.47
f1_churn['DecisionTree']= 0.46
# Visualisation

tn_dt, fp_dt, fn_dt, tp_dt = 4023, 616, 554, 484
cm_dt = [[tn_dt, fp_dt], [fn_dt, tp_dt]]

# Plot confusion matrix for Decision Tree
plt.figure(figsize=(6, 4))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Reds')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Decision Tree')
plt.show()
from functools import reduce

accuracy_df = pd.DataFrame(accuracy_scores.items(), columns=['Model', 'Accuracy'])
precision_df = pd.DataFrame(precision_churn.items(), columns=['Model', 'Precision on churn'])
recall_df = pd.DataFrame(recall_churn.items(), columns=['Model', 'Recall on churn'])
f1_df = pd.DataFrame(f1_churn.items(), columns=['Model', 'F1 on churn'])

dfs = [accuracy_df, precision_df, recall_df, f1_df]
result_table = reduce(lambda left, right: pd.merge(left, right, on='Model'), dfs)

print(result_table)
result_table = result_table.sort_values(by='Accuracy', ascending=False)
print(result_table)
Final Results

Recall on churn is our target value because this is most important to be identify. The models with high values are:

XGBClassifier with GridSearchCV – 0.47
DecisionTree – 0.47
Conclusion

The results are similar. There is one model with high accuracy but it fails to achieve high recall on churn. The most important is to achieve high result on recall on churn in order to identify clients with signs of leaving the bank. In my opinion we do not have enough information about customers behavior to have a good model to have acceptable results. Our model fail to achieve good enough result in order to identify churn customers.
Further Improvements

If this is our work, we need more information for the client’s behavior. Especially client dynamic behavior. For example:


*   Transaction count of different periods, Qs, months – tendences, changes

*   Turnovers – debit, credit - tendences, changes

*   CSI – changes – cross sell index/how many products of a bank the client uses/

*   Last time the client visited a bank branch

*   Last time the client visited a point of service – call center, online banking, mobile banking, ATM

*   How often client visited a branch – changes over a period

*   How often client visited a point of service – call center, online banking, mobile banking, ATM

*   Complains

*   Request for loans – approvals, rejections, dates

*   Request for service – debit card, credit card, online banking, insurance, pension fund

*   Problems with services – rejected transactions, problem with cards


*   Interaction with employees





Real-world Application

This exact model could not be implemented on Real-world Application because of its low recall on churn. On the other hand, the model could be improved with more features in order to achieve higher results. Then it could be implemented in bank in their Customer Value Management systems to identify the client ready to leave on different period basis. The clients could be contacted with offers or solutions of their problems in order to stay and achieve lower churn rate.


the above is what i wort 

